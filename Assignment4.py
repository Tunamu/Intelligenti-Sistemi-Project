import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from sklearn.model_selection import KFold
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

# ğŸ“Œ 1ï¸âƒ£ CSV DosyasÄ±nÄ± Oku
file_path = "big_pixel_counts_new_2.csv"
df = pd.read_csv(file_path)

# ğŸ“Œ 2ï¸âƒ£ Ã–zellikleri (X) ve Etiketleri (y) AyÄ±r
X = df.drop(columns=["Image", "Expected_Letter", "Ocp_Letter"]).values
y = df["Expected_Letter"].values

# ğŸ“Œ 3ï¸âƒ£ Etiketleri (Harfleri) SayÄ±ya Ã‡evir
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# ğŸ“Œ 4ï¸âƒ£ Veriyi Normalize Et
scaler = StandardScaler()
X = scaler.fit_transform(X)

# ğŸ“Œ 5ï¸âƒ£ 10-Fold Ã‡apraz DoÄŸrulama Ayarla
kf = KFold(n_splits=10, shuffle=True, random_state=42)
results = []

# ğŸ“Œ 6ï¸âƒ£ Scikit-Learn Modelleri
sklearn_models = {
    "SVM": SVC(),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "Logistic Regression": LogisticRegression(max_iter=5000),
    "KNN": KNeighborsClassifier(),
    "Gradient Boosting": GradientBoostingClassifier(),
    "MLP Neural Network": MLPClassifier(max_iter=3000)
}

# ğŸ“Œ 7ï¸âƒ£ PyTorch Dataset SÄ±nÄ±fÄ±nÄ± TanÄ±mla
class CustomDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.long)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# ğŸ“Œ 8ï¸âƒ£ PyTorch Modelleri
input_size = X.shape[1]
num_classes = len(label_encoder.classes_)

pytorch_models = {
    "MLP (PyTorch)": nn.Sequential(
        nn.Linear(input_size, 128), nn.ReLU(),
        nn.Linear(128, 64), nn.ReLU(),
        nn.Linear(64, num_classes)
    ),
    "CNN (PyTorch)": nn.Sequential(
        nn.Linear(input_size, 256), nn.ReLU(),
        nn.Linear(256, 128), nn.ReLU(),
        nn.Linear(128, num_classes)
    )
}

# ğŸ“Œ 9ï¸âƒ£ PyTorch Model EÄŸitimi Fonksiyonu
def train_model(model, train_loader, epochs=5):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    loss_fn = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    for epoch in range(epochs):
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = loss_fn(outputs, labels)
            loss.backward()
            optimizer.step()

# ğŸ“Œ ğŸ”Ÿ PyTorch Model DeÄŸerlendirme Fonksiyonu
def evaluate_model(model, test_loader):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()

    correct, total = 0, 0
    y_pred, y_true = [], []
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            predicted = torch.argmax(outputs, axis=1)
            correct += (predicted == labels).sum().item()
            total += labels.size(0)
            y_pred.extend(predicted.cpu().numpy())
            y_true.extend(labels.cpu().numpy())

    return round((correct / total) * 100, 2), np.array(y_true), np.array(y_pred)

# ğŸ“Œ 1ï¸âƒ£1ï¸âƒ£ 10-Fold Ã‡apraz DoÄŸrulama (TÃ¼m Modeller Ä°Ã§in)
for fold, (train_idx, test_idx) in enumerate(kf.split(X)):
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y_encoded[train_idx], y_encoded[test_idx]

    # ğŸ“Œ 1ï¸âƒ£2ï¸âƒ£ Scikit-Learn Modellerini EÄŸit ve Test Et
    for model_name, model in sklearn_models.items():
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        # Harf bazlÄ± doÄŸruluk hesapla
        accuracy_per_letter = {}
        for letter in np.unique(y_test):
            letter_mask = y_test == letter
            letter_accuracy = accuracy_score(y_test[letter_mask], y_pred[letter_mask])
            accuracy_per_letter[label_encoder.inverse_transform([letter])[0]] = round(letter_accuracy, 2)

        # SonuÃ§larÄ± kaydet
        result_entry = {"Fold": fold + 1, "Model": model_name}
        result_entry.update(accuracy_per_letter)
        results.append(result_entry)

    # ğŸ“Œ 1ï¸âƒ£3ï¸âƒ£ PyTorch Modellerini EÄŸit ve Test Et
    train_dataset = CustomDataset(X_train, y_train)
    test_dataset = CustomDataset(X_test, y_test)
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

    for model_name, model in pytorch_models.items():
        print(f"\nğŸš€ {model_name} - Fold {fold+1} EÄŸitiliyor...")
        train_model(model, train_loader)
        acc, y_true, y_pred = evaluate_model(model, test_loader)

        # PyTorch iÃ§in harf bazlÄ± doÄŸruluk hesaplama
        accuracy_per_letter = {}
        for letter in np.unique(y_true):
            letter_mask = y_true == letter
            letter_accuracy = accuracy_score(y_true[letter_mask], y_pred[letter_mask])
            accuracy_per_letter[label_encoder.inverse_transform([letter])[0]] = round(letter_accuracy, 2)

        # SonuÃ§larÄ± kaydet
        result_entry = {"Fold": fold + 1, "Model": model_name}
        result_entry.update(accuracy_per_letter)
        results.append(result_entry)

# ğŸ“Œ 1ï¸âƒ£4ï¸âƒ£ SonuÃ§larÄ± CSV'ye Kaydet
results_df = pd.DataFrame(results)
output_path = "combined_model_results.csv"
results_df.to_csv(output_path, index=False)

print(f"\nğŸ“Š SonuÃ§lar kaydedildi: {output_path}")
